# Spec: Game Engine & AI - Technical

- Version: v0.01 (Draft)
- Status: Draft
- Date: 2025-09-17
- Template: `docs/specs/spec-template_v1.1.md`
- Related: `spec-system-overview_v0.01.md`, `spec-api-functional_v0.01.md`, `spec-ui-functional_v0.01.md`, `spec-persistence-technical_v0.01.md`, `spec-observability-technical_v0.01.md`

---

## 1. Objectives
- Provide deterministic rules, state transitions, and outcome evaluation for Tic Tac Toe.
- Implement a simple state-action value function (Q-table) guiding AI move selection with epsilon-greedy exploration.
- Support end-of-game learning updates and batch self-play.

## 2. Domain Model
- Cell: enum `{ E, X, O }` (Empty, X, O) - API uses strings; engine uses enum internally.
- Board: array[9] of Cell in row-major order.
- Player: enum `{ X, O }`.
- GameState: `{ board, nextPlayer, status, moves }` where `status` ? `{ InProgress, WinX, WinO, Draw }`.
- Move: `{ index0to8 }` mapped from keypad 1-9.

## 3. Rules & Detection
- Legal move: target cell must be `E` and game `InProgress`.
- Win detection: 8 lines (3 rows, 3 cols, 2 diagonals).
- Draw detection: no empty cells and no win.
- Move ordering/indexing: keypad 1-9 maps to indices `[6,7,8,3,4,5,0,1,2]` (top-left is 6, top-right is 8).

## 4. AI Policy
- Q-table key: `stateKey` + `moveIndex0to8`.
  - `stateKey`: 9-char string with `X`,`O`,`E` from engine board.
- Action selection (epsilon = 0.15):
  - With probability epsilon choose a random legal move.
  - Else choose argmax over Q(s,a) among legal moves; break ties uniformly at random.
  - If no Q(s,·) exists, choose uniformly at random.
- Cold start: All Q(s,a) implicitly zero.

## 5. Learning Updates (end-of-game only)
- Outcome reward R:
  - Win: +1 applied to moves of winning side (but per requirements we update all moves with +1/-1 depending on winner/loser; draw gets small delta).
  - Loss: -1.
  - Draw: `delta = c * (4.5 - movesTaken) / 4.5` with `c = 0.2` for the game; apply to all moves.
- Scaling & bounds:
  - Apply learning rate alpha = 0.1: `Q <- clamp(Q + alpha * R, -5, +5)` for each updated (s,a).
  - Clamp Q to [-5, +5] after each update.
- Move attribution:
  - Update all moves regardless of who played them (as per decision); the sign of R comes from final result (+1 for winner's moves, -1 for loser's moves; draw uses `delta`).

## 6. State Encoding & Helpers
- `Encode(board)` -> `string[9]` for API; `ToStateKey(board)` -> 9-char `X/O/E` string for Q-table.
- `KeypadToIndex(k)` and `IndexToKeypad(i)` mapping utilities.
- Randomness: time-based by default; self-play accepts optional seed to seed `Random`.

## 7. Self-Play
- Run N games sequentially using current epsilon and policy.
- For each game: alternate turns based on starter; collect moves; apply end-of-game update; aggregate results.

## 8. Complexity & Limits
- State space is small; no symmetry reduction per decision.
- Q-table stored in-memory dictionary keyed by `(stateKey, moveIndex)`.
- Tie-breaking requires shuffling among equal max values.

## 9. Testing Strategy
- Unit tests: win/draw detection, legal moves, keypad mapping, epsilon-greedy selection, tie-break randomness bounds, Q-update math, clamping.
- Property tests: idempotent application of legal moves; no illegal moves generated by engine; end-of-game updates limited to bounds.
- Determinism: with fixed seed, self-play results should be reproducible.

## 10. Open Questions
- Consider exposing legal moves/hints API (optional).
- Consider switching to moving-average formula later for smoother convergence.

---

Change Log
- v0.01: Initial technical spec for engine, AI policy, and learning.
